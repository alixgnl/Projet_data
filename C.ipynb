{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import scipy.io as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "dataset = sp.loadmat('breastw.mat')\n",
    "\n",
    "X = dataset['X']\n",
    "y = dataset['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=y)\n",
    "\n",
    "# fit the model for outlier detection (default)\n",
    "clf = LocalOutlierFactor(n_neighbors=20, algorithm='auto', contamination=0.0001, metric='euclidean')\n",
    "# use fit_predict to compute the predicted labels of the training samples\n",
    "# (when LOF is used for outlier detection, the estimator has no predict,\n",
    "# decision_function and score_samples methods).\n",
    "#clf.fit(X_train,y_train)\n",
    "y_pred = clf.fit_predict(X_test)\n",
    "X_scores = -clf.negative_outlier_factor_\n",
    "\n",
    "y=np.reshape(y,(683,))\n",
    "#y_pred=np.reshape(y_pred,(683,1))\n",
    "y_pred[y_pred>0]=0\n",
    "y_pred[y_pred<0]=1\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1)\n",
    "result2 = accuracy_score(y_test,y_pred)\n",
    "print(\"Accuracy :\",result2)\n",
    "\n",
    "\n",
    "random_probs = [0 for _ in range(len(y_test))]\n",
    "# calculate AUC\n",
    "model_auc = roc_auc_score(y_test, X_scores)\n",
    "# summarize score\n",
    "print('Model: ROC AUC=%.3f' % (model_auc))\n",
    "# calculate ROC Curve\n",
    "    # For the Random Model\n",
    "random_fpr, random_tpr, _ = roc_curve(y_test, random_probs)\n",
    "    # For the actual model\n",
    "model_fpr, model_tpr, _ = roc_curve(y_test, X_scores)\n",
    "# Plot the roc curve for the model and the random model line\n",
    "plt.plot(random_fpr, random_tpr, linestyle='--', label='Random')\n",
    "plt.plot(model_fpr, model_tpr, marker='.', label='Model')\n",
    "# Create labels for the axis\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import scipy.io as sp\n",
    "\n",
    "dataset = sp.loadmat('breastw.mat')\n",
    "\n",
    "X = dataset['X']\n",
    "y = dataset['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=y)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X_test)\n",
    "y_pred=kmeans.labels_\n",
    "\n",
    "print(KMeans(n_clusters=2, random_state=0).fit(X_test))\n",
    "#X_scores ??\n",
    "\n",
    "y_test=np.reshape(y_test,(len(y_test,)))\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "result1 = classification_report(y_test, y_pred)\n",
    "result2 = accuracy_score(y_test,y_pred)\n",
    "if result2 < 0.1 :\n",
    "    y_pred[y_pred>0.5]=-1\n",
    "    y_pred[y_pred>-0.5]=1\n",
    "    y_pred[y_pred<0]=0\n",
    "    result1 = classification_report(y_test, y_pred)\n",
    "    result2 = accuracy_score(y_test,y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1)\n",
    "print(\"Accuracy :\",result2)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "random_probs = [0 for _ in range(len(y_test))]\n",
    "# calculate AUC\n",
    "model_auc = roc_auc_score(y_test, X_scores)\n",
    "# summarize score\n",
    "print('Model: ROC AUC=%.3f' % (model_auc))\n",
    "# calculate ROC Curve\n",
    "    # For the Random Model\n",
    "random_fpr, random_tpr, _ = roc_curve(y_test, random_probs)\n",
    "    # For the actual model\n",
    "model_fpr, model_tpr, _ = roc_curve(y_test, X_scores)\n",
    "# Plot the roc curve for the model and the random model line\n",
    "plt.plot(random_fpr, random_tpr, linestyle='--', label='Random')\n",
    "plt.plot(model_fpr, model_tpr, marker='.', label='Model')\n",
    "# Create labels for the axis\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import scipy.io as sp\n",
    "\n",
    "dataset = sp.loadmat('breastw.mat')\n",
    "\n",
    "X = dataset['X']\n",
    "y = dataset['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, stratify=y)\n",
    "y_test=np.reshape(y_test,(len(y_test),))\n",
    "\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X_test)\n",
    "X_scores=kde.score_samples(X_test)\n",
    "\n",
    "y_pred=X_scores\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(result)\n",
    "result1 = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\",)\n",
    "print (result1)\n",
    "result2 = accuracy_score(y_test,y_pred)\n",
    "print(\"Accuracy :\",result2)\n",
    "\"\"\"\n",
    "\n",
    "random_probs = [0 for _ in range(len(y_test))]\n",
    "# calculate AUC\n",
    "model_auc = roc_auc_score(y_test, X_scores)\n",
    "# summarize score\n",
    "print('Model: ROC AUC=%.3f' % (model_auc))\n",
    "# calculate ROC Curve\n",
    "    # For the Random Model\n",
    "random_fpr, random_tpr, _ = roc_curve(y_test, random_probs)\n",
    "    # For the actual model\n",
    "model_fpr, model_tpr, _ = roc_curve(y_test, X_scores)\n",
    "# Plot the roc curve for the model and the random model line\n",
    "plt.plot(random_fpr, random_tpr, linestyle='--', label='Random')\n",
    "plt.plot(model_fpr, model_tpr, marker='.', label='Model')\n",
    "# Create labels for the axis\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3b8f723d779205b1293d6866fcd213f57cba382b172edf103a84a699eabc50b9"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
